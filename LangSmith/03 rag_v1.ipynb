{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "RAG apps have two big failure modes:\n",
        "1. Retriever errors - wrong/irrelevant docs retrieved.\n",
        "2. Generator errors - model hallucinates or misuses context.\n",
        "\n",
        "In production, it's often unclear where the failure happened. Was the retriever bad, or did the LLM ignore\n",
        "the docs?\n",
        "\n",
        "LangSmith automatically records:\\\n",
        "    - User query\\\n",
        "    - Retrived documents\\\n",
        "    - LLM prompt (with inserted docs)\\\n",
        "    - LLM response"
      ],
      "metadata": {
        "id": "flrwjFO9yRvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries Installation\n",
        "# pip install langchain_google_genai --quiet --exists-action i --no-input\n",
        "# pip install langchain --quiet --exists-action i --no-input\n",
        "# pip install langchain-community faiss-cpu pypdf\n",
        "# pip install -qU langchain\n",
        "\n",
        "# Authenticate User\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Import libraries\n",
        "import os\n",
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.tracers.langchain import LangChainTracer # <-- Import the specific Tracer class\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "\n",
        "# Configure Environment Variables\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "# os.environ[\"LANGCHAIN_PROJECT\"] = \"Sequential LLM App\" # setting up newer one Tracing projects, however sometime older project may get considered as environment like Google Colab stores Variables at initial run and may ignore in next run so we can explictly pass it during invoking process\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# DEFINE MODEL\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-lite\",\n",
        "    project=userdata.get(\"GOOGLE_CLOUD_PROJECT\"),\n",
        "    location=\"global\",\n",
        "    temperature=0,\n",
        "    vertexai=True\n",
        ")\n",
        "\n",
        "# Create a tracer for the specific project\n",
        "tracer = LangChainTracer(project_name=\"RAG_Chatbot_v1\")\n",
        "\n",
        "# Create a custom configurations\n",
        "custom_configurations = {\n",
        "    \"callbacks\": [tracer],  # <--- This forces the LangSmith project name\n",
        "}\n",
        "\n",
        "\n",
        "# Locate the PDF\n",
        "PDF_PATH = \"/content/islr.pdf\"  # <-- change to your PDF filename\n",
        "\n",
        "# 1) Load PDF\n",
        "loader = PyPDFLoader(PDF_PATH)\n",
        "docs = loader.load()  # one Document per page\n",
        "\n",
        "# 2) Chunk\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "splits = splitter.split_documents(docs)\n",
        "\n",
        "# 3) Embed + index\n",
        "emb = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
        "vs = FAISS.from_documents(splits, emb)\n",
        "retriever = vs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
        "\n",
        "# 4) Prompt\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer ONLY from the provided context. If not found, say you don't know.\"),\n",
        "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
        "])\n",
        "\n",
        "# 5) Chain\n",
        "def format_docs(docs): return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "parallel = RunnableParallel(\n",
        "    {\n",
        "        \"context\": retriever | RunnableLambda(format_docs),\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        ")\n",
        "\n",
        "chain = parallel | prompt | llm | StrOutputParser()\n",
        "\n",
        "# 6) Ask questions\n",
        "print(\"PDF RAG ready. Ask a question (or type `exit`, `quit`, `q` to exit).\")\n",
        "while True:\n",
        "    q = input(\"Q: \")\n",
        "\n",
        "    if q.strip().lower() in [\"exit\", \"quit\", \"q\"]:\n",
        "        print(f\"Exiting... as recived {q.strip().lower()}\")\n",
        "        break\n",
        "\n",
        "    else:\n",
        "        print(\"Thinking...\")\n",
        "        ans = chain.invoke(q.strip(),config=custom_configurations)\n",
        "        print(\"A:\", ans)\n",
        "        print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gk25Qo17AoA",
        "outputId": "c45d45ef-9209-4481-91be-57038d1d4e75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF RAG ready. Ask a question (or type `exit`, `quit`, `q` to exit).\n",
            "Q: summarize the book in 50 words\n",
            "Thinking...\n",
            "A: This book offers an accessible introduction to statistical learning, covering key modeling and prediction techniques like linear regression, classification, and support vector machines. It uses real-world examples and R tutorials to help practitioners apply these methods in various fields.\n",
            "\n",
            "Q: who wrote this book?\n",
            "Thinking...\n",
            "A: The book was written by Gareth James, Daniela Witten, and Robert Tibshirani.\n",
            "\n",
            "Q: q\n",
            "Exiting... as recived q\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But here are some challenges like:\n",
        "- You can not trace a normal Python function like loading the PDF, chunking, embeddnig, prompting as we are not using Runnable.\n",
        "- and main issue is if we run the code again and again it will keep on loading the PDF and then will generate an embeddings and then users will send queries on it which is not a good practice\n",
        "    - to avoid this we kept user in loop but its not much logical and practical approach"
      ],
      "metadata": {
        "id": "E5EiqhxHbzRE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1_4WWzCckdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}