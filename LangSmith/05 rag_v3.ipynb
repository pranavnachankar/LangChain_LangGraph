{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In the previous code we faced the challenge that is two separate `run_name` are getting created under previous tracing projectName.\n",
        "\n",
        "so to resolve this :\n",
        "-  with @traceable use projectName too\n",
        "- also use while invokeing `invoke()` pass the `run_name` into that `custom_configurations`."
      ],
      "metadata": {
        "id": "JbR5RokTnQ5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries Installation\n",
        "# pip install -qU langchain_google_genai langchain langchain-community faiss-cpu pypdf langsmith\n",
        "\n",
        "# Authenticate User\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure Environment Variables\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG_Chatbot\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Import libraries\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.tracers.langchain import LangChainTracer\n",
        "from langsmith import traceable\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda"
      ],
      "metadata": {
        "id": "7Gk25Qo17AoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DEFINE MODEL\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-lite\",\n",
        "    project=userdata.get(\"GOOGLE_CLOUD_PROJECT\"),\n",
        "    location=\"global\",\n",
        "    temperature=0,\n",
        "    vertexai=True\n",
        ")"
      ],
      "metadata": {
        "id": "A1_4WWzCckdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a tracer for the specific project\n",
        "tracer = LangChainTracer(project_name=\"RAG_Chatbot\")\n",
        "\n",
        "# Create a custom configurations\n",
        "custom_configurations = {\n",
        "    \"callbacks\": [tracer],\n",
        "    \"run_name\" : \"pdf_rag_query\"\n",
        "}"
      ],
      "metadata": {
        "id": "RuLVayOXw1vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Locate the PDF\n",
        "PDF_PATH = \"/content/islr.pdf\"\n",
        "\n",
        "# ---------- traced setup steps ----------\n",
        "# We explicitly pass project_name to ensure it uses the correct project even if env vars are sticky\n",
        "# we can add tags and metadata too in @traceable\n",
        "# ----------------- helpers (not traced individually) -----------------\n",
        "@traceable(name=\"load_pdf\", project_name=\"RAG_Chatbot\")\n",
        "def load_pdf(path: str):\n",
        "    loader = PyPDFLoader(path)\n",
        "    return loader.load()  # list[Document]\n",
        "\n",
        "@traceable(name=\"split_documents\", project_name=\"RAG_Chatbot\")\n",
        "def split_documents(docs, chunk_size=1000, chunk_overlap=150):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_documents(docs)\n",
        "\n",
        "@traceable(name=\"build_vectorstore\", project_name=\"RAG_Chatbot\")\n",
        "def build_vectorstore(splits):\n",
        "    emb = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
        "    vs = FAISS.from_documents(splits, emb)\n",
        "    return vs\n",
        "\n",
        "# ----------------- parent setup function (traced) -----------------\n",
        "@traceable(name=\"setup_pipeline\", project_name=\"RAG_Chatbot\")\n",
        "def setup_pipeline(pdf_path: str, chunk_size=1000, chunk_overlap=150):\n",
        "    docs = load_pdf(PDF_PATH)\n",
        "    splits = split_documents(docs)\n",
        "    vs = build_vectorstore(splits)\n",
        "    return vs\n",
        "\n",
        "\n",
        "# ---------- pipeline ----------\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer ONLY from the provided context. If not found, say you don't know.\"),\n",
        "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
        "])\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "\n",
        "# ----------------- one top-level (root) run -----------------\n",
        "@traceable(name=\"pdf_rag_full_run\", project_name=\"RAG_Chatbot\")\n",
        "def setup_pipeline_and_query(pdf_path: str, question: str):\n",
        "    vectorstore = setup_pipeline(PDF_PATH, chunk_size=1000, chunk_overlap=150)\n",
        "\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type=\"similarity\",\n",
        "        search_kwargs={\"k\": 4}\n",
        "    )\n",
        "\n",
        "    parallel = RunnableParallel(\n",
        "        {\n",
        "        \"context\": retriever | RunnableLambda(format_docs),\n",
        "        \"question\": RunnablePassthrough(),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    chain = parallel | prompt | llm | StrOutputParser()\n",
        "    ans = chain.invoke(q, config=custom_configurations)\n",
        "    print(\"\\nA:\", ans)"
      ],
      "metadata": {
        "id": "Yle-nyRNwt3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------- run a query ----------\n",
        "print(\"PDF RAG ready. Ask a question (or Ctrl+C to exit).\")\n",
        "q = input(\"\\nQ: \").strip()\n",
        "\n",
        "setup_pipeline_and_query(pdf_path=PDF_PATH, question=q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhIPspWf8bco",
        "outputId": "fcba423e-465d-4b40-8a9b-bad4ef760fe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF RAG ready. Ask a question (or Ctrl+C to exit).\n",
            "\n",
            "Q: who is the authers?\n",
            "\n",
            "A: The authors are Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sgarvjvb8rQM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}