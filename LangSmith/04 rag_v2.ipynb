{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Here we will work on the challenge faced in previous RAG application :\n",
        "\n",
        "-  Missing of traces for run_name of custom python functions\n",
        "\n",
        "\n",
        "With the help of `@traceable` we can trace those custom python functions."
      ],
      "metadata": {
        "id": "JbR5RokTnQ5H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries Installation\n",
        "# pip install -qU langchain_google_genai langchain langchain-community faiss-cpu pypdf langsmith\n",
        "\n",
        "# Authenticate User\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure Environment Variables\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"RAG_Chatbot\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"GEMINI_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Import libraries\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.tracers.langchain import LangChainTracer\n",
        "from langsmith import traceable\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
        "\n",
        "\n",
        "# DEFINE MODEL\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-lite\",\n",
        "    project=userdata.get(\"GOOGLE_CLOUD_PROJECT\"),\n",
        "    location=\"global\",\n",
        "    temperature=0,\n",
        "    vertexai=True\n",
        ")\n",
        "\n",
        "# Create a tracer for the specific project\n",
        "tracer = LangChainTracer(project_name=\"RAG_Chatbot\")\n",
        "\n",
        "# Create a custom configurations\n",
        "custom_configurations = {\n",
        "    \"callbacks\": [tracer],\n",
        "    \"run_name\" : \"pdf_rag_query\"\n",
        "}\n",
        "\n",
        "# Locate the PDF\n",
        "PDF_PATH = \"/content/islr.pdf\"\n",
        "\n",
        "# ---------- traced setup steps ----------\n",
        "# We explicitly pass project_name to ensure it uses the correct project even if env vars are sticky\n",
        "@traceable(name=\"load_pdf\", project_name=\"RAG_Chatbot\")\n",
        "def load_pdf(path: str):\n",
        "    loader = PyPDFLoader(path)\n",
        "    return loader.load()  # list[Document]\n",
        "\n",
        "@traceable(name=\"split_documents\", project_name=\"RAG_Chatbot\")\n",
        "def split_documents(docs, chunk_size=1000, chunk_overlap=150):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_documents(docs)\n",
        "\n",
        "@traceable(name=\"build_vectorstore\", project_name=\"RAG_Chatbot\")\n",
        "def build_vectorstore(splits):\n",
        "    emb = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
        "    vs = FAISS.from_documents(splits, emb)\n",
        "    return vs\n",
        "\n",
        "@traceable(name=\"setup_pipeline\", project_name=\"RAG_Chatbot\")\n",
        "def setup_pipeline(pdf_path: str):\n",
        "    docs = load_pdf(PDF_PATH)\n",
        "    splits = split_documents(docs)\n",
        "    vs = build_vectorstore(splits)\n",
        "    return vs\n",
        "\n",
        "# ---------- pipeline ----------\n",
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer ONLY from the provided context. If not found, say you don't know.\"),\n",
        "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
        "])\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(d.page_content for d in docs)\n",
        "\n",
        "# Build the index\n",
        "vectorstore = setup_pipeline(PDF_PATH)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
        "\n",
        "parallel = RunnableParallel({\n",
        "    \"context\": retriever | RunnableLambda(format_docs),\n",
        "    \"question\": RunnablePassthrough(),\n",
        "})\n",
        "\n",
        "chain = parallel | prompt | llm | StrOutputParser()\n",
        "\n",
        "# ---------- run a query ----------\n",
        "print(\"PDF RAG ready. Ask a question (or Ctrl+C to exit).\")\n",
        "q = input(\"\\nQ: \").strip()\n",
        "\n",
        "ans = chain.invoke(q, config=custom_configurations)\n",
        "print(\"\\nA:\", ans)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Gk25Qo17AoA",
        "outputId": "b67d55de-2eee-4352-c187-6f2e9d05dfd8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF RAG ready. Ask a question (or Ctrl+C to exit).\n",
            "\n",
            "Q: what is linear regression?\n",
            "\n",
            "A: Linear regression is a straightforward approach for predicting a quantitative response on the basis of a single predictor variable. It assumes that there is approximately a linear relationship between the predictor variable and the response.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1_4WWzCckdj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}